### Hi there 👋 I'm Shawn. Currently working as a LLM practitioner.
发起项目主要有大模型训练，和大模型代码解析(逐行)两块。还有一些重大项目的微小贡献。  
[Baichuan微调训练](https://github.com/ArtificialZeng/Baichuan-Chat-Tuning)       *** [百川微调训练源码解析](https://github.com/ArtificialZeng/Baichuan-Qwen-Llama-tuning-Explained)  
[ChatGLM2-6B-Explained](https://github.com/ArtificialZeng/ChatGLM2-6B-Explained) ***  [ChatGLM-Efficient-Tuning-Explained](https://github.com/ArtificialZeng/ChatGLM-Efficient-Tuning-Explained)

Source Code Contributor/源码贡献项目：  
[llama3](https://github.com/meta-llama/llama3) - 21.5k star Meta开源的世界上最强大的开源模型  
[Qwen](https://github.com/QwenLM/Qwen) - 11.7k star 阿里出品千问7B/14B同级别SOTA chat项目  
[Baichuan2](https://github.com/baichuan-inc/Baichuan2) - 4k star 百川7-13B版本的第二个版本(9.6发布)  
[RAGFlow](https://github.com/infiniflow/ragflow) - 8.7k star RAG近万星框架  
fastllm - 3.1k star 大模型加速/量化  
Awesome-Prompt-Engineering - 3.4k star 提示工程合集  
PaddleNLP - 11.6k 百度出品的通用NLP框架  
Pycaret - 8.6k 顶级自动化机器学习框架  
AutoX - 自动化机器学习  
[ProG](https://github.com/sheldonresearch/ProG/tree/main) - 图神经网络框架  
ailabx（getee）- 量化金融  
torchlm 等

The initiated projects mainly include large model training and large model code analysis (line by line).   
There are also minor contributions to major projects.   
Baichuan fine-tuning training *** Baichuan fine-tuning training source code analysis 
ChatGLM2-6B-Explained *** 
ChatGLM-Efficient-Tuning-Explained Source Code Contributor

<!--
**ArtificialZeng/ArtificialZeng** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- 🔭 I’m currently working on ...
- 🌱 I’m currently learning ...
- 👯 I’m looking to collaborate on ...
- 🤔 I’m looking for help with ...
- 💬 Ask me about ...
- 📫 How to reach me: ...
- 😄 Pronouns: ...
- ⚡ Fun fact: ...
-->
